# -*- coding: utf-8 -*-
"""ResNet-CIFAR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13yiNWzOV-vOu6YsDFTWJVC6tj68PRJxw

abstract
Intro
method
result

Initial parameters:


1.   11,173,962
2.
"""



"""# Deep Learning Mini-Project

Team members:

1. Nishith Sharma
2. Utkarsh Atri
3. Parth Bharadwaj

# Custom ResNet for CiFar dataset
Objective: In this mini-project you are tasked with coming up with a modified residual network (ResNet)
architecture with the highest test accuracy on the CIFAR-10 image classification dataset, under the
constraint that your model has no more than 5 million parameters.

# 1. Load Libraries
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.backends.cudnn as cudnn
import torchvision
import torchvision.transforms as transforms
import os
import argparse

"""# 2. Build the Basic Block for ResNet:
A basic ResNet block is composed by two layers of 3x3 conv/batchnorm/relu. In the picture, the lines represent the residual operation. The dotted line means that the shortcut was applied to match the input and the output dimension.
>>
![basicBlock.png](data:image/png;base64,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)


"""

class BasicBlock(nn.Module):

  def __init__(self, in_planes, planes, kernel_size, skip_kernel, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=kernel_size, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=kernel_size, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, planes, kernel_size=skip_kernel, stride=stride, bias=False),
                nn.BatchNorm2d(planes)
            )

  def forward(self, x):
      out = F.relu(self.bn1(self.conv1(x)))
      out = self.bn2(self.conv2(out))
      out += self.shortcut(x)
      out = F.relu(out)
      return out

"""# 3. ResNet Class with customizable components:
* No. of Residual Layers
* No. of Residual Blocks in Residual Layer i
* No. of channels in Residual Layer i
* Conv. kernel size in Residual Layer i
* Skip connection kernel size in Residual Layer i
* Average pool kernel size
* layers container
* strides for layers
"""

class ResNet(nn.Module):

    def __init__(self,N:int, B:list, C:list, F:list, K:list, P:int, num_classes=10):
        super(ResNet, self).__init__()
        self.in_planes = C[0]
        self.block = BasicBlock
        self.N = N                # No. of Residual Layers
        self.B = B                # No. of Residual Blocks in Residual Layer i
        self.C = C                # No. of channels in Residual Layer i
        self.F = F                # Conv. kernel size in Residual Layer i
        self.K = K                # Skip connection kernel size in Residual Layer i
        self.P = P                # Average pool kernel size
        self.layers = []          # layers container
        self.S = [2] * N          # strides for layers
        self.S[0] = 1

        # Output Liner layer input dimension
        self.outLayerInSize = C[N-1]*(32//(P*2**(N-1)))*(32//(P*2**(N-1)))

        # Print Model Config
        print("\n\nModel Config: "
            "\n-------------------------------------"
            "\nN (# Layers)\t:",self.N,
            "\nB (# Blocks)\t:",self.B,
            "\nC (# Channels)\t:",C,
            "\nF (Conv Kernel)\t:",F,
            "\nK (Skip Kernel)\t:",K,
            "\nP (Pool Kernel)\t:",P,)

        self.conv1 = nn.Conv2d(3, C[0], kernel_size=F[0], stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(C[0])
        for i in range(N): 
            exec("self.layer{} = self._make_layer(self.block, self.C[{}], self.B[{}], self.F[{}], self.K[{}], self.S[{}])"\
                .format(i+1,i,i,i,i,i))
            exec("self.layers.append(self.layer{})".format(i+1))
        self.linear = nn.Linear(self.outLayerInSize, num_classes)
        

    def _make_layer(self, block, planes, num_blocks, kernel_size, skip_kernel, stride):
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, kernel_size, skip_kernel, stride))
            self.in_planes = planes
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        for layer in self.layers:
            out = layer(out)
        out = F.avg_pool2d(out, self.P)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out

"""# 4. Custom ResNet Model Configurations
Here we will test around with various forms of parameters to assess which one gives the best accuracy for total parameters to be under 5 million.
"""

def custom_model():

    B=[3,3,2,3]           # Bi : # Residual blocks in Residual Layer i
    C=[64,128,128,256]    # Ci : # channels in Residual Layer i
    F=[3,3,3,3]           # Fi : Conv. kernel size in Residual Layer i
    K=[1,1,1,1]           # Ki : Skip connection kernel size in Residual Layer i
    P=4                   # P  : Average pool kernel size
    N=len(B)              # N: : # Residual Layers

    return ResNet(N, B, C, F, K, P)     # Return the custom model class

def ResNet10_1():

    N=4
    B=[1,1,1,1]
    C=[64,128,256,512]
    F=[3,3,3,3]
    K=[1,1,1,1]
    P=4

    return ResNet(N, B, C, F, K, P)

def ResNet10_2():

    N=3
    B=[1,1,1]
    C=[32,64,128]
    F=[3,3,3]
    K=[1,1,1]
    P=4

    return ResNet(N, B, C, F, K, P)

def ResNet24_2():

    N=3
    B=[3,3,3]
    C=[64,128,256]
    F=[3,3,3]
    K=[1,1,1]
    P=4

    return ResNet(N, B, C, F, K, P)


def ResNet34_1():

    N=4
    B=[3,4,6,3]
    C=[64,128,256,512]
    F=[3,3,3,3]
    K=[1,1,1,1]
    P=4

    return ResNet(N, B, C, F, K, P)


def ResNet16_1():

    B=[2,1,2,2]
    C=[64,128,256,256]
    F=[3,3,3,3]
    K=[1,1,1,1]
    P=4
    N=len(B)

    return ResNet(N, B, C, F, K, P)


def ResNet24_1():

    B=[3,3,2,3]
    C=[64,128,128,256]
    F=[3,3,3,3]
    K=[1,1,1,1]
    P=4
    N=len(B)

    return ResNet(N, B, C, F, K, P)


def ResNet48_1():

    B=[8,5,5,5]
    C=[64,128,128,128]
    F=[3,3,3,3]
    K=[1,1,1,1]
    P=4
    N=len(B)

    return ResNet(N, B, C, F, K, P)


def ResNet_test():

    B=[1,1,1,1]
    C=[64,128,256,512]
    F=[3,3,3,3]
    K=[1,1,1,1]
    P=4
    N=len(B)

    return ResNet(N, B, C, F, K, P)


def ResNetXYZ():

    B=[1,1,1]
    C=[64,128,128]
    F= [3,3,3]
    K= [1,1,1]
    P=4
    N=len(B)

    return ResNet(N, B, C, F, K, P)


def ResNet86():

    B= [10,8,10,14]
    C= [16,32,64,128]
    F= [3,3,3,3]
    K= [1,1,1,1]
    P= 4
    N= len(B)

    return ResNet(N, B, C, F, K, P)


def ResNet12():

    B= [2,1,1,1]
    C= [64,128,256,512]
    F= [3,3,3,3]
    K= [1,1,1,1]
    P= 4
    N= len(B)

    return ResNet(N, B, C, F, K, P)


def ResNet32():

    B= [4,4,4,3]
    C= [32,64,128,256]
    F= [3,3,3,3]
    K= [1,1,1,1]
    P= 4
    N= len(B)

    return ResNet(N, B, C, F, K, P)


def ResNet8():

    B= [1,1,1]
    C= [128,256,512]
    F= [3,3,3]
    K= [1,1,1]
    P= 4
    N= len(B)

    return ResNet(N, B, C, F, K, P)

def Proposed_model():

    B=[4,4,4,4]           # Bi : # Residual blocks in Residual Layer i
    C=[64,128,256,512]    # Ci : # channels in Residual Layer i
    F=[3,3,3,3]           # Fi : Conv. kernel size in Residual Layer i
    K=[2,2,2,2]           # Ki : Skip connection kernel size in Residual Layer i
    P=3                   # P  : Average pool kernel size
    N=len(B)              # N: : # Residual Layers

    return ResNet(N, B, C, F, K, P)     # Return the custom model class

"""# 5. Load and prepare data"""

# Data
print('==> Preparing data..')
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

trainset = torchvision.datasets.CIFAR10(
    root='./data', train=True, download=True, transform=transform_train)
trainloader = torch.utils.data.DataLoader(
    trainset, batch_size=128, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(
    root='./data', train=False, download=True, transform=transform_test)
testloader = torch.utils.data.DataLoader(
    testset, batch_size=100, shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat', 'deer',
           'dog', 'frog', 'horse', 'ship', 'truck')

"""# 6. Define Train and Test functions"""

# Commented out IPython magic to ensure Python compatibility.
# Training
def train(epoch):
    print('\nEpoch: %d' % epoch)
    net.train()
    train_loss = 0
    correct = 0
    total = 0
    for batch_idx, (inputs, targets) in enumerate(trainloader):
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'
#                      % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))

# Commented out IPython magic to ensure Python compatibility.
#Testing
def test(epoch):
    global best_acc
    net.eval()
    test_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_idx, (inputs, targets) in enumerate(testloader):
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = net(inputs)
            loss = criterion(outputs, targets)

            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'
#                          % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))

    # Save checkpoint.
    acc = 100.*correct/total
    if acc > best_acc:
        print('Saving..')
        state = {
            'net': net.state_dict(),
            'acc': acc,
            'epoch': epoch,
        }
        if not os.path.isdir('checkpoint'):
            os.mkdir('checkpoint')
        torch.save(state, './checkpoint/ckpt.pth')
        best_acc = acc

"""# 7. Building the model"""

lr = 0.1           # Define the learning rate

device = 'cuda' if torch.cuda.is_available() else 'cpu'

best_acc = 0       # best test accuracy
start_epoch = 0    # start from epoch 0 or last checkpoint epoch

# Model
print('==> Building model..')
net = custom_model()

net = net.to(device)
if device == 'cuda':
    net = torch.nn.DataParallel(net)
    cudnn.benchmark = True

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.1,
                      momentum=0.9, weight_decay=5e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)



from torchsummary import summary
summary(ResNet8().cuda(),(3,32,32))

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'The model has {count_parameters(net):,} trainable parameters')

count_parameters(ResNet8())

"""# 8. Train the model"""

for epoch in range(start_epoch, start_epoch+200):
    train(epoch)
    test(epoch)
    scheduler.step()

"""# 9. Results"""

print("The best accuracy after training :", best_acc)
print(f'The model has {count_parameters(net):,} trainable parameters')

"""# 10. Additional Experiments

Here we have added the code and the other experiments to eventually come to the above solutions. We used various configurations to find the model with params under 5 million and pick the one with maximum accuracy.

Note: In order to save time we have conducted 4 experiments in this notebook and the rest of the experiments were conducted in other notebook to utilize time efficiently. We have included the results of other expirements in the report.
"""



print('==> Experiment1 ..')
net = ResNet8()

net = net.to(device)
if device == 'cuda':
    net = torch.nn.DataParallel(net)
    cudnn.benchmark = True

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.1,
                      momentum=0.9, weight_decay=5e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'The model has {count_parameters(net):,} trainable parameters')

for epoch in range(start_epoch, start_epoch+25):
    train(epoch)
    test(epoch)
    scheduler.step()

print('==> Experiment1 ..')
net = ResNet12()

net = net.to(device)
if device == 'cuda':
    net = torch.nn.DataParallel(net)
    cudnn.benchmark = True

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.1,
                      momentum=0.9, weight_decay=5e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'The model has {count_parameters(net):,} trainable parameters')

for epoch in range(start_epoch, start_epoch+25):
    train(epoch)
    test(epoch)
    scheduler.step()

print('==> Experiment2 ..')
net = ResNet16_1()

net = net.to(device)
if device == 'cuda':
    net = torch.nn.DataParallel(net)
    cudnn.benchmark = True

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.1,
                      momentum=0.9, weight_decay=5e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'The model has {count_parameters(net):,} trainable parameters')

for epoch in range(start_epoch, start_epoch+25):
    train(epoch)
    test(epoch)
    scheduler.step()

print('==> Experiment3 ..')
net = ResNet_test()

net = net.to(device)
if device == 'cuda':
    net = torch.nn.DataParallel(net)
    cudnn.benchmark = True

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.1,
                      momentum=0.9, weight_decay=5e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'The model has {count_parameters(net):,} trainable parameters')

for epoch in range(start_epoch, start_epoch+25):
    train(epoch)
    test(epoch)
    scheduler.step()

# Failed Experiment

# class BasicBlock(nn.Module):
#     expansion = 1

#     def __init__(self, in_planes, planes, stride=1):
#         super(BasicBlock, self).__init__()
#         self.conv1 = nn.Conv2d(
#             in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
#         self.bn1 = nn.BatchNorm2d(planes)
#         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,
#                                stride=1, padding=1, bias=False)
#         self.bn2 = nn.BatchNorm2d(planes)

#         self.shortcut = nn.Sequential()
#         if stride != 1 or in_planes != self.expansion*planes:
#             self.shortcut = nn.Sequential(
#                 nn.Conv2d(in_planes, self.expansion*planes,
#                           kernel_size=1, stride=stride, bias=False),
#                 nn.BatchNorm2d(self.expansion*planes)
#             )

#     def forward(self, x):
#         out = F.relu(self.bn1(self.conv1(x)))
#         out = self.bn2(self.conv2(out))
#         out += self.shortcut(x)
#         out = F.relu(out)
#         return out

# class Bottleneck(nn.Module):
#     expansion = 4

#     def __init__(self, in_planes, planes, stride=1):
#         super(Bottleneck, self).__init__()
#         self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
#         self.bn1 = nn.BatchNorm2d(planes)
#         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,
#                                stride=stride, padding=1, bias=False)
#         self.bn2 = nn.BatchNorm2d(planes)
#         self.conv3 = nn.Conv2d(planes, self.expansion *
#                                planes, kernel_size=1, bias=False)
#         self.bn3 = nn.BatchNorm2d(self.expansion*planes)

#         self.shortcut = nn.Sequential()
#         if stride != 1 or in_planes != self.expansion*planes:
#             self.shortcut = nn.Sequential(
#                 nn.Conv2d(in_planes, self.expansion*planes,
#                           kernel_size=1, stride=stride, bias=False),
#                 nn.BatchNorm2d(self.expansion*planes)
#             )

#     def forward(self, x):
#         out = F.relu(self.bn1(self.conv1(x)))
#         out = F.relu(self.bn2(self.conv2(out)))
#         out = self.bn3(self.conv3(out))
#         out += self.shortcut(x)
#         out = F.relu(out)
#         return out

# class ResNet(nn.Module):
#     def __init__(self, block, num_blocks, num_classes=10):
#         super(ResNet, self).__init__()
#         self.in_planes = 64

#         self.conv1 = nn.Conv2d(3, 64, kernel_size=3,
#                                stride=1, padding=1, bias=False)
#         self.bn1 = nn.BatchNorm2d(64)
#         self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
#         self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
#         self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
#         self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
#         self.linear = nn.Linear(512*block.expansion, num_classes)

#     def _make_layer(self, block, planes, num_blocks, stride):
#         strides = [stride] + [1]*(num_blocks-1)
#         layers = []
#         for stride in strides:
#             layers.append(block(self.in_planes, planes, stride))
#             self.in_planes = planes * block.expansion
#         return nn.Sequential(*layers)

#     def forward(self, x):
#         out = F.relu(self.bn1(self.conv1(x)))
#         out = self.layer1(out)
#         out = self.layer2(out)
#         out = self.layer3(out)
#         out = self.layer4(out)
#         out = F.avg_pool2d(out, 4)
#         out = out.view(out.size(0), -1)
#         out = self.linear(out)
#         return out


# def ResNet18():
#     return ResNet(BasicBlock, [1, 1, 1, 1])


# def ResNet34():
#     return ResNet(BasicBlock, [3, 4, 6, 3])


# def ResNet50():
#     return ResNet(Bottleneck, [3, 4, 6, 3])


# def ResNet101():
#     return ResNet(Bottleneck, [3, 4, 23, 3])


# def ResNet152():
#     return ResNet(Bottleneck, [3, 8, 36, 3])


# def test():
#     net = ResNet18()
#     y = net(torch.randn(1, 3, 32, 32))
#     print(y.size())